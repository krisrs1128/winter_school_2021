[
  {
    "path": "posts/2021-12-15-session1-2/",
    "title": "Preprocessing and Featurization",
    "description": {},
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\nA few simple improvements to the quality of a dataset can make a much bigger impact in downstream modeling performance than even the most extensive search across the fanciest models. For this reason, in real-world modeling (not to mention competitions), significant time is dedicated to data exploration and preprocessing1 In these notes, we’ll review some of the most common changes we can make to a dataset before we feed it into a model.\nFirst, a caveat: Never overwrite the raw data! Instead, write a workflow that transforms the raw data into preprocessed data. This makes it possible to try a few different transformations and see how the resulting performances compare. Also, we want to have some confidence that the overall analysis is reproducible, and if we make irreversible changes to the raw data, then we lose all chance at that.\nPreprocessing\nOutliers: A single point very far away from the rest can result in poor fits. For example, when fitting a linear regression, the whole line can be thrown off by one point,\n(picture)\nThere are many approaches to detecting outliers. However, a simple rule of thumb is to compute the interquartile range for each feature (the difference between the 75% and 25% percentiles). Look for points that are more than 1.5 or 2 times this range away from the median. They are potential outliers.\nDifferences in scales: Sparse regression is sensitive to the units used to train them (so is deep learning). For example, this means that changing the units of a certain input from meters to millimeters will have a major influence on the fitted model. To address this issue, it is common to try rescaling each variable. For example, subtracting the mean and dividing by the standard deviation will ensure all the variables have approximately the same range. It’s also possible to convert any variable \\(x_{d}\\) to the range \\(\\left[0, 1\\right]\\) by using the transformation \\(\\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\\).\nMissing values: For some types of data, features have to be dropped for some samples. The sensor might have a temporary breakdown, or a respondent might have skipped a question, for example. Unfortunately, most methods do not have natural ways for handling missingness. To address this, it’s possible to impute a plausible value for all the missing values. A standard approach to imputation is to replace the missing values in a column using the median of all observed values in that column. This is called median imputation.\nMore sophisticated approaches are able to learn the relationships across columns, and use these correlations to learn better imputations. This is called median imputation.\nBe especially careful with numerical values that have been used as a proxy for missingness (e.g., sometimes -99 is used as a placeholder). These should not be treated as actual numerical data, they are in fact missing!\nCategorical inputs: From the last set of notes, only tree-based methods can directly use categorical variables as inputs. For other methods, the categorical variables needed to be coded. For example, variables with two levels can be converted to 0’s and 1’s, and variables with \\(K\\) levels can be one-hot coded,\n\nSometimes a variable has many levels. For example, a variable might say which city a user was from. In some cases, a few categories are very common, with the rest appearing only a few times. For the first situation, one solution is to replace the level of the category with the average value of the response within that category – this is called response coding. For the second, it’s possible to lump all the rare categories into an “other” column.\nTo detect the types of issues discussed here, a good practice is to compute\nsummary statistics and (ii) a histogram / barplot for every feature in the dataset. Even if there are a few hundred features, it’s possible to skim the associated summaries relatively quickly (just a few seconds for each).\nFeaturization\nOften, useful predictors can be derived from, but are not explicitly present in, the raw data. For example, linear regression would struggle with either of the cases below,\nbut if we add new columns to the original data \\(x\\), we will be able to fit the response well. The reason is that, even though \\(y\\) is not linear in \\(x\\), it is linear in the derived features \\(\\mathbf{1}\\left\\{x > 0\\}\\) and \\(x^2\\), respectively.\nThis idea applies to all sorts of data, not just numerical columns,\nLongitudinal measurements: Imagine we want to classify a patient’s recovery probability based on a series of lab test results. We don’t have to use the raw measurements, we can use the trend, the range, or the “spikiness,” for example.\nText: We can convert a document into the counts for different words.\nImages: For some tasks, the average color can be predictive (imagine classifying scenes as either forest or beach, which do you think will have more green on average?).\nThe difficulty of deriving useful features in image data was one of the original motivations for deep learning. By automatically learning relevant features, deep learning replaced a whole suite of more complicated image feature extractors (e.g., HOG and SIFT).\n\nEven Andrej Karpathy calls it step 1.↩︎\n",
    "preview": {},
    "last_modified": "2021-12-16T09:48:41-06:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Setting Up Supervised Learning Probems",
    "description": {},
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2021-12-16",
    "categories": [],
    "contents": "\n\n\n\nWe are making predictions all the time, often without realizing it. For example, imagine we are waiting at a bus stop and want to guess how long it will be before a bus arrives. We can combine many sources of evidence,\nHow many people are currently at the stop? If there are more people, we think a bus might arrive soon.\nWhat time of day is it? If it’s during rush hour, we would expect more frequent service.\nWhat is the weather like? If it is poor weather, we might expect delays.\netc.\nTo think about the process formally, we could imagine a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^{D}\\) reflecting \\(D\\) characteristics of our environment. If we collected data about how long we actually had to wait, call it \\(y_i\\), for every day in a year, then we would have a dataset \\[\\begin{align*}\n\\left(\\mathbf{x}_1, y_1\\right) \\\\\n\\left(\\mathbf{x}_2, y_2\\right) \\\\\n\\vdots \\\\\n\\left(\\mathbf{x}_{365}, y_{365}\\right) \\\\\n\\end{align*}\\] and we could try to summarize the relationship \\(\\mathbf{x}_i \\to y_i\\). Methods for making this process automatic, based simply on a training dataset, are called supervised learning methods.\nIn the above example, the inputs were a mix of counts (number of people at stop?) and categorical (weather) data types, and our response was a nonnegative continuous value. In general, we could have arbitrary data types for either input or response variable. A few types of outputs are so common that they come with their own names,\n\\(y_i\\) continuous \\(\\to\\) regression\n\\(y_i\\) categorical \\(\\to\\) classification\nFor example,\nTrying to determine whether a patient’s disease will be cured by a treatment is a classification problem – the outcomes are either yes, they will be cured, or no, they won’t.\nTrying to estimate the crop yield of a plot of farmland based on a satellite image is a regression problem – it could be any continuous, nonnegative number.\nThere are in fact many other types of responses (ordinal, multiresponse, survival, functional, image-to-image, …) each which come with their own names and set of methods, but for our purposes, it’s enough to focus on regression and classification.\nThere is a nice geometric way of thinking about supervised learning. For regression, think of the inputs on the \\(x\\)-axis and the response on the \\(y\\)-axis. Regression then becomes the problem of estimating a one-dimensional curve from data.\n\n\n\nIn higher-dimensions, this becomes a surface.\n\n\n\nIf some of the inputs are categorical (e.g., poor vs. good weather), then the regression function is no longer a continuous curve, but we can still identify group means.\nClassification has a similar geometric interpretation, except instead of a continuous response, we have categorical labels. We can associate classes with colors. If we have only one input, classification is the problem of learning which regions of the input are associated with certain colors.\n\n\n\nIn higher-dimensions, the view is analogous. We just want to find boundaries between regions with clearly distinct colors. For example, for disease recurrence, blood pressure and resting heart rate might be enough to make a good guess about whether a patient will have recurrence or not.\n\n\n\nModel Classes\nDrawing curves and boundaries sounds simple, but is a surprisingly difficult problem, especially when the number of potentially informative features \\(D\\) is large. It helps to have predefined types of curves (and boundaries) that we can refer to and use to partially automate the process of supervised learning. We’ll call an example of these predefined curve types a “model class.” In later notes, we’ll discuss how exactly to choose one member of each model class that fits a dataset well. For now, let’s just build some intuition about what each model class looks like and how we might be able to fit it with data.\nLinear Models\nMaybe the simplest curve is a linear one, \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1.\n\\end{align*}\\] Here, \\(b_0\\) gives the \\(y\\)-intercept and \\(b_1\\) gives the slope. When we have many input features, the equivalent formula is \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\] where I’ve used the dot-product from linear algebra to simplify notation (after having appended a 1). This kind of model is called a linear regression model.\nHow do we find a \\(b\\) that fits the data well? We can try to optimize a “loss” function. This measures the quality of the fitted line. For linear regression, a good choice is squared error loss,\n\\[\\begin{align*}\nL\\left(b\\right) = \\sum_{i = 1}^{N} \\left(y_i - b^{T}x_{i}\\right)^2.\n\\end{align*}\\]\nFor classification, we can imagine drawing a linear boundary. For simplicity, we’ll assume we have only two classes, though a similar partition of the space can be made for arbitrary numbers of classes.\nTo describe this, we need to define a direction \\(b\\) perpendicular to the boundary. We will say that whenever \\[\\begin{align*}\nf_{b}\\left(x\\right) = \\frac{1}{1 + \\text{exp}\\left(x^{T}b\\right)}\n\\end{align*}\\] is larger than 0.5, we’re in the orange region, and whenever it’s smaller than 0.5, we’re in the purple region. This kind of model is called a logistic regression model.\nWe need a loss function for logistic regression too. In theory, we could continue to use squared error loss, but we can do better by considering the fact that the true response is only one of two values. To make things concrete, say that \\(y_i = 1\\) whenever it is an orange point, otherwise \\(y_i = 0\\). We can use binary cross-entropy loss,\n\\[\\begin{align*}\n\\sum_{i = 1}^{N} y_i \\log\\left(f_{b}\\left(x_i\\right)\\right) + \\left(1 - y_i\\right) \\log\\left(1 - f_{b}\\left(x_i\\right)\\right)\n\\end{align*}\\]\nTo understand this loss, let’s plot one term as a function of \\(f_{b}\\left(x_i\\right)\\), both when \\(y_i\\) is 1 and when it is 0.\n(draw picture)\nLet’s fit a linear regression in code. Below, I’m loading a dataset on ….. Notice that the response variable is continuous,\nLet’s do the same thing for a logistic regression.\nSparse Linear Models\nIn many cases, we will have recorded many types of features – coordinates of \\(x_{i}\\) – that are not actually related to the response. A model that knows to ignore certain of the features will do better than a model that tries to use all of them. This is the main idea behind using sparsity in linear regression. We again fit the model \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\] but we make the assumption that many of the \\(b_{d}\\) are exactly 0. Graphically, we imagine that the response does not change at all as we change some of the inputs, all else held equal.\nThis is how we implement the approach in sklearn.\nThe same idea can be applied to logistic regression. In this case, having a coefficient \\(b_d = 0\\) means that the probabilities for different class labels do not change at all as features \\(x_d\\) is changed.\nHere is an implementation in sklearn.\nTree-based Models\nTree-based models fit a different class of curves. To motivate them, consider making a prediction for the bus time arrival problem using the following diagram,\nNotice that we can use the same logic to do either regression or classification. For regression, we associate each “leaf” at the bottom of the tree with a continuous prediction. For classification, we associate it with the probability for different classes. It turns out that we can train these models using squared error and cross-entropy losses as before, though the details are beyond the scope of these notes.\nIt’s not immediately obvious, but these rules are equivalent to drawing curves that are piecewise constant over subsets of the input space. Let’s convince ourselves using some pictures. First, notice that a tree with a single split is exactly a “curve” that takes on two values, depending on the split point,\nIf we split the same variable deeper, it creates more steps,\nWhat if we had two variables? Depending on the order, of the splits, we create different axis-aligned partitions,\nQ: What would be the diagram if I had switched \\(x_1\\) and \\(x_2\\) in the tree?\nA very common variation on tree-based models computes a large ensemble of trees and then combines their curves in some way. How exactly they are combined is beyond the scope of these notes, but this is what random forests and gradient boosted decision trees are doing in the background.\nWe can implement these models in sklearn using the following code.\nRelationships across classes\nThe diagram below summarizes the relationships across classes.\nWhen should we use which of these approaches? Here are some relative strengths and weaknesses.\n\nStrengths\nWeaknesses\nLinear / Logistic Regression\n* Often to easy interpret* No tuning parameters* Very fast to train\n* Unstable when many features to pick from* Can only fit linear curves / boundaries (though, see featurization notes)\nSparse Linear / Logistic Regression\n* Often easy to interpret* Stable even when many features to pick from* Very fast to train\n* Can only fit linear curves / boundaries\nTree-based Classification / Regression\n* Can fit nonlinear functions of inputs\n* Can be slow to train* Somewhat harder to interpret\nHere is a flow chart you can use to guide the choice of which model class to use when.\nFor example, try matching models to responses in the examples below,\nQ1: We want to predict whether a patient has a disease given just the genetic profile. There are 1000 genes that can serve as predictors. There are only two possible responses.\nQ2: A user on a site has been watching (too many…) episodes of Doctor Who. How many more minutes will they remain on the site today? As predictors, you have their features of their current and past viewing behavior (e.g., current time of day, number of hours per week for each of the last 4 weeks, etc.). We suspect that there are important nonlinear relationships between these predictors and the response.\nQ3: We are trying to predict the next hour’s total energy production in a wind farm. We have a years worth of past production and weather data, but right now, we just want a baseline using current wind speed and the last hour’s production.\nThe answers are,\nA1: Sparse logistic regression. We have two classes, and most of the genes are unlikely to be relevant for classification.\nA2: A tree-based method, like random forests or gradient boosting. This is because we anticipate a nonlinear relationship.\nA3: Linear regression. The response is continuous and we just need a baseline using two predictors.\n\n\n\n",
    "preview": "posts/welcome/figures/curve-1d.png",
    "last_modified": "2021-12-16T10:53:23-06:00",
    "input_file": "welcome.knit.md"
  },
  {
    "path": "posts/2021-12-15-session1-3/",
    "title": "Overfitting, Bias-Variance, and Cross-Validation",
    "description": "Choosing models with the right complexity.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\nEven if we can effectively draw curves through data, we might not actually be able to make good predictions on future samples. For example, suppose we have the dataset below for training a model,\nWe could fit this curve,\nand it would get zero error on this training dataset. However, it won’t do nearly as well on future observations as the this fit,\nThis issue can also occur in classification. For example, the decision boundary here,\nis perfect on the training dataset. But it will likely perform worse than the simpler boundary here,\nThis phenomenon is called overfitting. Complex models might appear to do well on a dataset available for training, only to fail when they are released “in the wild” to be applied to new samples. We need to be very deliberate about choosing models that are complex enough to fit the essential structure in a dataset, but not so complex that they overfit to patterns that will not appear again in future samples.\nThe simplest way to choose a model with the right level of complexity is to use data splitting. Randomly split the data that are available into a train and a test sets. Fit a collection of models, all of different complexities, on the training set. Look at the performances of those models on the test set. Choose the model with the best performance on the test set.\nA more sophisticated approach is to use cross-validation. Instead of using only a single train / test split, we can group the data into \\(K\\)-different “folds.” For each complexity level , we can train \\(K\\) models, each time leaving out one of the folds. Performance of these models on the hold-out folds is used for estimating the appropriate complexity level.\nThere are a few subtleties related to train / test splits and cross-validation that are worth knowing,\nWe want the held-out data to be as representative of new, real-world data as possible. Sometimes, randomly split data will not be representative. For example, maybe the data are collected over time. A more “honest” split would split data into past vs. future samples starting at a few different timepoints, training on past and evaluating on future. The sample issue occurs if we plan on applying a model to a new geographic context or market segment, for example.\nCross-validation can be impractical on larger datasets. For this reason, we often see only a single train / test split for evaluation of algorithms in large data settings.\nWe incur a bias when splitting the data. The fact that we split the data means that we aren’t using all the available data, which leads to slightly lower performance. If it hurts more complex models more than simpler ones, than it can even lead to slightly incorrect selection. After having chosen a given model complexity, it’s worth retraining that model on the full available dataset.\nLet’s implement both simple train / test and cross-validation using sklearn. Notice that training error is much lower than test error.\nRelated to overfitting, there is an important phenomenon called the bias-variance tradeoff. To understand this trade-off, imagine being able to sample hundreds of datasets similar to the one that is available. On each of these imaginary datasets, we can imagine training a new model (at a given complexity level). Then, we can define,\nBias: Average predictions from models trained across each of the imaginery datasets. How far off are they from the truth, on average?\nVariance: How similar are predictions from across the different models?\nIdeally, we would have low values for both bias and variance, since they both contribute to performance on the test set. In practice, though, there is a trade-off. Often, the models that tend to be closest to the truth on average might be far off on any individual run.\nConversely, models that are very stable from run to run might be consistenly incorrect in certain regions.\nModels with high variance but low bias tend to be overfit, and models with low variance but high bias tend to be underfit. Models that have good test or cross-validation errors have found a good compromise between bias and variance.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T09:48:46-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-15-session1-4/",
    "title": "Model Evaluation and Visualization",
    "description": "Communicating model behavior after fitting them.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\nIn some cases, it might be enough to know that a model is giving the correct predictions most of the time, without worrying too much about how it arrived at that decision. If a medical classifier worked 99.9% of the time, I wouldn’t question it – I would try to figure out what to do next in the treatment plan. That said, in many situations, we might want to attribute model performance to a few characteristics of the data. This is often the case when we are using a model as a proxy for understanding a little more about the (biological, business, environmental, political, social, …) processes that generated the data. In these notes, we’ll review a few types of summaries and visualizations that can help with these attribution tasks.\nFor (sparse) linear and logistic regression models, we can inspect the fitted coefficients \\(b\\). If we have standardized the data, then the coefficients with the largest coefficients lead to the largest change in the response / class probabilities, all else held equal. In particular, coefficients that are estimated to be 0 can be safely ignored.\nNote that the fitted coefficient \\(\\hat{b}_{d}\\) should be interpreted in context of all the variables in the model, not just the \\(d^{th}\\). This is because \\(\\hat{b}_{d}\\) represents the effect of variable \\(d\\), after having controlled for the rest. To convince yourself that this important, you could try simulating data with two correlated predictors and seeing how the fitted coefficients compare to the uncorrelated predictor case.\nFor tree-based models, there are no coefficients \\(\\hat{b}\\) to use for interpretation. Instead, it’s common to consider the variable importance statistic. The importance of a variable measures the deterioration in a model when that variable is removed. If the model deteriorates substantially, then that variable is said to be important.\nLet’s see how we extract coefficients and variable importance measures in sklearn.\nError Analysis\nEarlier, we discussed using a test set or cross-validation to evaluate how well a model performs on new data. A related, but deeper, question is to understand on which types of samples a model does better or worse. That is, instead of just asking how well a model performs on average, we may want to characterize situations where a model performs well and situations where it performs poorly. This can be used both to summarize the limitations of a model and guide future improvements.\nError Quantiles: One strategy to error analysis is to take samples from different quantiles of the error distribution and then summarize their associated features. Specifically, it’s possible to create a histogram of the squared error or cross-entropy losses on test samples. Take samples in the far right tail – these are those with very large loss. What characteristics do they tend to share?\nError Prediction: A related approach is to use the sample-level test set errors from the previous step and use them as features in a new model from a different model class. The features that help distinguish low and high-error samples can be used to identify regions of poor performance. Note that, if it’s possible to do better than a random baseline, then there is systematic structure remaining in the data that has not been used by the original model class.\nVisualizations\nAs more complex models become more common in practice, visualization has emerged as a key way for (a) summarizing their essential structure and (b) motivating further modeling refinements. We will discuss the role of ceteris paribus and partial dependence profiles.\nWe will write \\(x^{d \\vert = z}\\) to denote the observation \\(x\\) with the \\(d^{th}\\) coordinate set to \\(z\\). How should we describe the effect of changing the \\(d^{th}\\) input on predictions made by complex models \\(f\\left(x\\right)\\)? As a thought experiment, consider the example below. The surface is the fitted function \\(f(x)\\), mapping a two dimensional input \\(x\\) to a continuous response. How would you summarize the relationship between \\(x_1\\) and \\(y\\)? The main problem is that the shape of the relationship depends on which value of \\(x_2\\) we start at.\nOne idea is to consider the values of \\(x_2\\) that were observed in our dataset. Then, we can evaluate our model over a range of values \\(x_1\\) after fixing those values of \\(x_2\\). These curves are called Ceteris Paribus (CP) profiles The same principle holds in higher dimensions. We can fix \\(D−1\\) coordinates of an observation and then evaluate what happens to a sample’s predictions when we vary coordinate \\(d\\). Mathematically, this is expressed by \\(h_{x}^{f, d}\\left(z\\right) := f\\left(\\mathbf{x}^{d\\vert= z}\\right)\\)\nTo summarize a set of CP profiles, we can take their average across the dataset. This is called a partial dependence (PD) profile. It is a more concise alternative to CP profiles, showing one curve per features, rather than one curve per sample.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-16T09:48:52-06:00",
    "input_file": {}
  }
]
