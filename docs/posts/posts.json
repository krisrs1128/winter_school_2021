[
  {
    "path": "posts/welcome/",
    "title": "Setting Up Supervised Learning Probems",
    "description": {},
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2021-12-15",
    "categories": [],
    "contents": "\nWe are making predictions all the time, often without realizing it. For example, imagine we are waiting at a bus stop and want to guess how long it will be before a bus arrives. We can combine many sources of evidence,\nHow many people are currently at the stop? If there are more people, we think a bus might arrive soon.\nWhat time of day is it? If it’s during rush hour, we would expect more frequent service.\nWhat is the weather like? If it is poor weather, we might expect delays.\netc.\nTo think about the process formally, we could imagine a vector \\(x_i \\in \\mathbb{R}^{D}\\) reflecting \\(D\\) characteristics of our environment. If we collected data about how long we actually had to wait, call it \\(y_i\\), for every day in a year, then we would have a dataset \\[\\begin{align*}\n\\left(x_1, y_1\\right) \\\\\n\\left(x_2, y_2\\right) \\\\\n\\vdots \\\\\n\\left(x_{365}, y_{365}\\right) \\\\\n\\end{align*}\\] and we could try to summarize the relationship \\(x_i \\to y_i\\). Methods for making this process automatic, based simply on a training dataset, are called supervised learning methods.\nIn the above example, the inputs were a mix of counts (number of people at stop?) and categorical (weather) data types, and our response was a nonnegative continuous value. In general, we could have arbitrary data types for either input or response variable. A few types of outputs are so common that they come with their own names,\n\\(y_i\\) continuous \\(\\to\\) regression\n\\(y_i\\) categorical \\(\\to\\) classification\nFor example, trying to determine whether a patient’s disease will be cured by a treatment is a classification problem – the outcomes are either yes, they will be cured, or no, they won’t. Trying to estimate the crop yield of a plot of farmland based on a satellite image is a regression problem – it could be any continuous, nonnegative number. There are in fact many other types of responses (ordinal, multiresponse, survival, functional, image-to-image, …) each which come with their own names and set of methods, but for our purposes, it’s enough to focus on regression and classification.\nThere is a nice geometric way of thinking about supervised learning. For regression, think of the inputs on the \\(x\\)-axis and the response on the \\(y\\)-axis. Regression then becomes the problem of estimating a one-dimensional curve from data.\nIn higher-dimensions, this becomes a surface.\nIf some of the inputs are categorical (e.g., poor vs. good weather), then the regression function is no longer a continuous curve, but we can still identify group means.\nClassification has a similar geometric interpretation, except instead of a continuous response, we have categorical labels. We can associate classes with colors. If we have only one input, classification is the problem of learning which regions of the input are associated with certain colors.\nIn higher-dimensions, the view is analogous. We just want to find boundaries between regions with clearly distinct colors. For example, for disease recurrence, blood pressure and resting heart rate might be enough to make a good guess about whether a patient will have recurrence or not.\nModel Classes\nDrawing curves and boundaries sounds simple, but is a surprisingly difficult problem, especially when the number of potentially informative features \\(D\\) is large. It helps to have predefined types of curves (and boundaries) that we can refer to and use to partially automate the process of supervised learning. We’ll call an example of these predefined curve types a “model class.” In later notes, we’ll discuss how exactly to choose one member of each model class that fits a dataset well. For now, let’s just build some intuition about what each model class looks like and how we might be able to fit it with data.\nLinear Models\nMaybe the simplest curve is a linear one, \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1.\n\\end{align*}\\] Here, \\(b_0\\) gives the \\(y\\)-intercept and \\(b_1\\) gives the slope. When we have many input features, the equivalent formula is \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\] where I’ve used the dot-product from linear algebra to simplify notation (after having appended a 1). This kind of model is called a linear regression model.\nHow do we find a \\(b\\) that fits the data well? We can try to optimize a “loss” function. This measures the quality of the fitted line. For linear regression, a good choice is squared error loss,\n\\[\\begin{align*}\nL\\left(b\\right) = \\sum_{i = 1}^{N} \\left(y_i - b^{T}x_{i}\\right)^2.\n\\end{align*}\\]\nFor classification, we can imagine drawing a linear boundary. For simplicity, we’ll assume we have only two classes, though a similar partition of the space can be made for arbitrary numbers of classes.\nTo describe this, we need to define a direction \\(b\\) perpendicular to the boundary. We will say that whenever \\[\\begin{align*}\nf_{b}\\left(x\\right) = \\frac{1}{1 + \\text{exp}\\left(x^{T}b\\right)}\n\\end{align*}\\] is larger than 0.5, we’re in the orange region, and whenever it’s smaller than 0.5, we’re in the purple region. This kind of model is called a logistic regression model.\nWe need a loss function for logistic regression too. In theory, we could continue to use squared error loss, but we can do better by considering the fact that the true response is only one of two values. To make things concrete, say that \\(y_i = 1\\) whenever it is an orange point, otherwise \\(y_i = 0\\). We can use binary cross-entropy loss,\n\\[\\begin{align*}\n\\sum_{i = 1}^{N} y_i \\log\\left(f_{b}\\left(x_i\\right)\\right) + \\left(1 - y_i\\right) \\log\\left(1 - f_{b}\\left(x_i\\right)\\right)\n\\end{align*}\\]\nTo understand this loss, let’s plot one term as a function of \\(f_{b}\\left(x_i\\right)\\), both when \\(y_i\\) is 1 and when it is 0.\n(draw picture)\nLet’s fit a linear regression in code. Below, I’m loading a dataset on ….. Notice that the response variable is continuous,\nLet’s do the same thing for a logistic regression.\nSparse Linear Models\nIn many cases, we will have recorded many types of features – coordinates of \\(x_{i}\\) – that are not actually related to the response. A model that knows to ignore certain of the features will do better than a model that tries to use all of them. This is the main idea behind using sparsity in linear regression. We again fit the model \\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\] but we make the assumption that many of the \\(b_{d}\\) are exactly 0. Graphically, we imagine that the response does not change at all as we change some of the inputs, all else held equal.\nThis is how we implement the approach in sklearn.\nThe same idea can be applied to logistic regression. In this case, having a coefficient \\(b_d = 0\\) means that the probabilities for different class labels do not change at all as features \\(x_d\\) is changed.\nHere is an implementation in sklearn.\nTree-based Models\nTree-based models fit a different class of curves. To motivate them, consider making a prediction for the bus time arrival problem using the following diagram,\nNotice that we can use the same logic to do either regression or classification. For regression, we associate each “leaf” at the bottom of the tree with a continuous prediction. For classification, we associate it with the probability for different classes. It turns out that we can train these models using squared error and cross-entropy losses as before, though the details are beyond the scope of these notes.\nIt’s not immediately obvious, but these rules are equivalent to drawing curves that are piecewise constant over subsets of the input space. Let’s convince ourselves using some pictures. First, notice that a tree with a single split is exactly a “curve” that takes on two values, depending on the split point,\nIf we split the same variable deeper, it creates more steps,\nWhat if we had two variables? Depending on the order, of the splits, we create different axis-aligned partitions,\nQ: What would be the diagram if I had switched \\(x_1\\) and \\(x_2\\) in the tree?\nA very common variation on tree-based models computes a large ensemble of trees and then combines their curves in some way. How exactly they are combined is beyond the scope of these notes, but this is what random forests and gradient boosted decision trees are doing in the background.\nWe can implement these models in sklearn using the following code.\nRelationships across classes\nThe diagram below summarizes the relationships across classes.\nWhen should we use which of these approaches? Here are some relative strengths and weaknesses.\n\nStrengths\nWeaknesses\nLinear / Logistic Regression\n* Often to easy interpret* No tuning parameters* Very fast to train\n* Unstable when many features to pick from* Can only fit linear curves / boundaries (though, see featurization notes)\nSparse Linear / Logistic Regression\n* Often easy to interpret* Stable even when many features to pick from* Very fast to train\n* Can only fit linear curves / boundaries\nTree-based Classification / Regression\n* Can fit nonlinear functions of inputs\n* Can be slow to train* Somewhat harder to interpret\n\n\n\n",
    "preview": {},
    "last_modified": "2021-12-15T19:04:54-06:00",
    "input_file": {}
  }
]
