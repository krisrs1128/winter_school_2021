---
title: "Setting Up Supervised Learning Probems"
description: | 
author:
  - name: Kris Sankaran
    url: https://krisrs1128.github.com/LSLab
    affiliation: UW Madison
    affiliation_url: https://stat.wisc.edu
output:
  distill::distill_article:
    self_contained: false
---

1. We are making predictions all the time, often without realizing it. For
example, imagine we are waiting at a bus stop and want to guess how long it will
be before a bus arrives. We can combine many sources of evidence,
    * How many people are currently at the stop? If there are more people, we think
    a bus might arrive soon.
    * What time of day is it? If it's during rush hour, we would expect more
    frequent service.
    * What is the weather like? If it is poor weather, we might expect delays.
    * etc.
    
    To think about the process formally, we could imagine a vector $x_i \in \mathbb{R}^{D}$ reflecting $D$ characteristics of our environment. 
If we collected data about how long we actually had to wait, call it $y_i$, for
every day in a year, then we would have a dataset
\begin{align*}
\left(x_1, y_1\right) \\
\left(x_2, y_2\right) \\
\vdots \\
\left(x_{365}, y_{365}\right) \\
\end{align*}
  and we could try to summarize the relationship $x_i \to y_i$. Methods for making
this process automatic, based simply on a training dataset, are called
supervised learning methods.

2. In the above example, the inputs were a mix of counts (number of people at
stop?) and categorical (weather) data types, and our response was a nonnegative
continuous value. In general, we could have arbitrary data types for either
input or response variable. A few types of outputs are so common that they come
with their own names,
    * $y_i$ continuous $\to$ regression
    * $y_i$ categorical $\to$ classification
    
    For example, trying to determine whether a patient's disease will be cured by a
treatment is a classification problem -- the outcomes are either yes, they will
be cured, or no, they won't. Trying to estimate the crop yield of a plot of
farmland based on a satellite image is a regression problem -- it could be any
continuous, nonnegative number. There are in fact many other types of responses
(ordinal, multiresponse, survival, functional, image-to-image, ...) each which
come with their own names and set of methods, but for our purposes, it's enough
to focus on regression and classification.

3. There is a nice geometric way of thinking about supervised learning. For
regression, think of the inputs on the $x$-axis and the response on the
$y$-axis. Regression then becomes the problem of estimating a one-dimensional
curve from data.

    In higher-dimensions, this becomes a surface.
    
    If some of the inputs are categorical (e.g., poor vs. good weather), then the
regression function is no longer a continuous curve, but we can still identify
group means.

4. Classification has a similar geometric interpretation, except instead of a
continuous response, we have categorical labels. We can associate classes with
colors. If we have only one input, classification is the problem of learning
which regions of the input are associated with certain colors.

    In higher-dimensions, the view is analogous. We just want to find boundaries
between regions with clearly distinct colors. For example, for disease
recurrence, blood pressure and resting heart rate might be enough to make a good
guess about whether a patient will have recurrence or not.

### Model Classes

5. Drawing curves and boundaries sounds simple, but is a surprisingly difficult
problem, especially when the number of potentially informative features $D$ is
large. It helps to have predefined types of curves (and boundaries) that we can
refer to and use to partially automate the process of supervised learning. We'll
call an example of these predefined curve types a "model class." In later notes,
we'll discuss how exactly to choose one member of each model class that fits a
dataset well. For now, let's just build some intuition about what each model
class looks like and how we might be able to fit it with data.

#### Linear Models

6. Maybe the simplest curve is a linear one,
\begin{align*}
f_{b}\left(x\right) = b_0 + b_1 x_1.
\end{align*}
Here, $b_0$ gives the $y$-intercept and $b_1$ gives the slope. When we have many input features, the equivalent formula is
\begin{align*}
f_{b}\left(x\right) = b_0 + b_1 x_1 + \dots + b_{D}x_{D} := b^{T}x,
\end{align*}
where I've used the dot-product from linear algebra to simplify notation (after having appended a 1). This kind of model is called a _linear regression model_.

7. For classification, we can imagine drawing a linear boundary. 

To describe this, we need to define a direction $b$ perpendicular to the
boundary. We will say that whenever
\begin{align*}
f_{b}\left(x\right) = \frac{1}{1 + \text{exp}\left(x^{T}b\right)}
\end{align*}
is larger than 0.5, we're in the orange region, and whenever it's smaller than
0.5, we're in the purple region. Notice that this function is 0.5 exactly when
$x^{T}b = 0$. This kind of model is called a _logistic regression model_.

8. Let's fit a linear regression in code. Below, I'm loading a dataset on .....
Notice that the response variable is continuous,

9. Let's do the same thing for a logistic regression.

#### Sparse Linear Models

10. 


#### Tree-based Models

### Relationships across classes

Recording 1: Setting up supervised learning problems
3. Basic model classes
	1. Linear and logistic
		1. Graphical representations
		2. Mathematical setup
		3. Implementation in sklearn
	2. Sparse linear and logistic
		1. Graphical representations
		2. Mathematical setup
		3. Implementation in sklearn
	3. Tree-based
		1. Graphical representations
		2. Mathematical setup
		3. Implementation in sklearn
4. Relationships between model classes
	1. Concept map. Links showing l1 penalty for sparsity. trees for nonlinearity.
	2. Strengths vs. weaknesses