---
title: "Overfitting, Bias-Variance, and Cross-Validation"
description: |
    Choosing models with the right complexity.
author:
  - name: Kris Sankaran
    url: https://krisrs1128.github.com/LSLab
date: 12-15-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

1. Even if we can effectively draw curves through data, we might not actually be
able to make good predictions on future samples. For example, suppose we have
the dataset below for training a model,

We could fit this curve,


and it would get zero error on this _training dataset_. However, it won't do
nearly as well on future observations as the this fit,

2. This issue can also occur in classification. For example, the decision
boundary here,

is perfect on the training dataset. But it will likely perform worse than the
simpler boundary here,

3. This phenomenon is called _overfitting_. Complex models might appear to do
well on a dataset available for training, only to fail when they are released
"in the wild" to be applied to new samples. We need to be very deliberate about
choosing models that are complex enough to fit the essential structure in a
dataset, but not so complex that they overfit to patterns that will not appear
again in future samples.

4. The simplest way to choose a model with the right level of complexity is to
use data splitting. Randomly split the data that are available into a train and
a test sets. Fit a collection of models, all of different complexities, on the
training set. Look at the performances of those models on the test set. Choose
the model with the best performance on the test set.

5. A more sophisticated approach is to use cross-validation. Instead of using
only a single train / test split, we can group the data into $K$-different
"folds." For each complexity level , we can train $K$ models, each time leaving
out one of the folds. Performance of these models on the hold-out folds is used
for estimating the appropriate complexity level.

6. There are a few subtleties related to train / test splits and
cross-validation that are worth knowing,

  * We want the held-out data to be as representative of new, real-world data as
  possible. Sometimes, randomly split data will not be representative. For
  example, maybe the data are collected over time. A more "honest" split would
  split data into past vs. future samples starting at a few different
  timepoints, training on past and evaluating on future. The sample issue occurs
  if we plan on applying a model to a new geographic context or market segment,
  for example.
  * Cross-validation can be impractical on larger datasets. For this reason, we
  often see only a single train / test split for evaluation of algorithms in
  large data settings.
  * We incur a bias when splitting the data. The fact that we split the data
  means that we aren't using all the available data, which leads to slightly
  lower performance. If it hurts more complex models more than simpler ones,
  than it can even lead to slightly incorrect selection. After having chosen a
  given model complexity, it's worth retraining that model on the full available
  dataset.

7. Let's implement both simple train / test and cross-validation using
`sklearn`. Notice that training error is much lower than test error.

8. Related to overfitting, there is an important phenomenon called the
bias-variance tradeoff. To understand this trade-off, imagine being able to
sample hundreds of datasets similar to the one that is available. On each of
these imaginary datasets, we can imagine training a new model (at a given
complexity level). Then, we can define,

  * Bias: Average predictions from models trained across each of the imaginery
  datasets. How far off are they from the truth, on average?
  * Variance: How similar are predictions from across the different models?

Ideally, we would have low values for both bias and variance, since they both
contribute to performance on the test set. In practice, though, there is a
trade-off. Often, the models that tend to be closest to the truth on average
might be far off on any individual run.

Conversely, models that are very stable from run to run might be consistenly
incorrect in certain regions.

Models with high variance but low bias tend to be overfit, and models with low
variance but high bias tend to be underfit. Models that have good test or
cross-validation errors have found a good compromise between bias and variance.